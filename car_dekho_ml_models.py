# -*- coding: utf-8 -*-
"""Car_Dekho_ML_Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11UvOct7tSGlQ6kqXIt7SY2OdJH_G3iZr
"""

# Import libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error

from google.colab import files
uploaded = files.upload()  # This will prompt you to upload a file

# Load the uploaded CSV file
import pandas as pd
df_final = pd.read_csv('student_enc.csv')
print(df_final.head())

# statistical details of the data
df_final.describe()

# create x and y datas

x=df_final.drop(columns=["price"],axis=1)
y=df_final["price"]

# scaling the data with standardScaler
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
x_scaled=scaler.fit_transform(x)

# Import necessary libraries
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns



# function to choose the best algorithm

def best_ML_algorithm(x,y,algorithms):

        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)

        results = []

        for algorithm in algorithms:

                model=algorithm().fit(x_train,y_train)
                y_pred=model.predict(x_test)
                MSE_1=mean_squared_error(y_test,y_pred)
                MAE_1=mean_absolute_error(y_test,y_pred)
                RMSE_1=np.sqrt(mean_squared_error(y_test,y_pred))
                r_squr_1=r2_score(y_test,y_pred)
                results.append({"model": type(model).__name__,
                                          "MAE": MAE_1,
                                          "MSE": MSE_1,
                                          "RMSE": RMSE_1,
                                           "R2": r_squr_1 })
                print(f"Trainscore: {model.score(x_train, y_train)}")
                print(f"Testscore: {model.score(x_test, y_test)}")
                # Plot Actual vs Predicted Prices
                plt.figure(figsize=(10, 6))
                sns.scatterplot(x=y_test, y=y_pred)
                plt.xlabel('Actual Prices')
                plt.ylabel('Predicted Prices')
                plt.title(f'{type(model).__name__}: Actual vs Predicted Prices')
                plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')  # Add reference line
                plt.show()
        results_df = pd.DataFrame(results)

        return results_df

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor


# using above function try to identify the best algorithm

algorithms = [LinearRegression, DecisionTreeRegressor, RandomForestRegressor,GradientBoostingRegressor]

results_df = best_ML_algorithm(x_scaled,y, algorithms)

# model comparision
results_df

"""## L1 and L2 Regularization to Prevent **overfitting**"""

from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd

# Split data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(x_scaled, y, test_size=0.25, random_state=42)

# Define alpha values to test
alpha_values = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]
values = []

# Train Ridge models for each alpha
for i in alpha_values:
    # Using solver='saga' to avoid sym_pos-related issues
    ridgeModel = Ridge(alpha=i, solver='saga', random_state=42)
    ridgeModel.fit(X_train, Y_train)
    y_pred = ridgeModel.predict(X_test)
    r_squr = r2_score(Y_test, y_pred)
    values.append(r_squr)

# Find the best alpha value
best_alpha_ridge = alpha_values[np.argmax(values)]
print(f"Best alpha for Ridge Regression: {best_alpha_ridge}")

# Train Ridge model with the best alpha
ridgeModel_1 = Ridge(alpha=best_alpha_ridge, solver='saga', random_state=42)
ridgeModel_1.fit(X_train, Y_train)
y_pred_ridge = ridgeModel_1.predict(X_test)

# Evaluate the model
MSE_ridge = mean_squared_error(Y_test, y_pred_ridge)
MAE_ridge = mean_absolute_error(Y_test, y_pred_ridge)
RMSE_ridge = np.sqrt(MSE_ridge)
r_squr_ridge = r2_score(Y_test, y_pred_ridge)

# Create a DataFrame to view the evaluation metrics
ridge_results = pd.DataFrame({
    "model": ["RidgeRegressor"],
    "Best Alpha": [best_alpha_ridge],
    "MAE": [MAE_ridge],
    "MSE": [MSE_ridge],
    "RMSE": [RMSE_ridge],
    "R2": [r_squr_ridge]
})


ridge = pd.DataFrame({
    "model": ["RidgeRegressor"],
    "MAE": [MAE_ridge],
    "MSE": [MSE_ridge],
    "RMSE": [RMSE_ridge],
    "R2": [r_squr_ridge]
})

print(ridge_results)

# using ridge to fit and evalutes the model
from sklearn.linear_model import Lasso
alpha_values_L = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]
values_L=[]
for i in alpha_values_L:
    LassoModel = Lasso(alpha=i)
    LassoModel.fit(X_train,Y_train)
    y_pred=LassoModel.predict(X_test)
    r_squr=r2_score(Y_test,y_pred)
    values_L.append(r_squr)
print(values_L)

# best alpha values for prediction
best_alpha_lasso = alpha_values_L[np.argmax(values_L)]
LassoModel_1 = Lasso(alpha=best_alpha_lasso)
LassoModel_1.fit(X_train, Y_train)
y_pred_lasso=LassoModel_1.predict(X_test)

# model evaluation
MSE_lasso=mean_squared_error(Y_test,y_pred_lasso)
MAE_lasso=mean_absolute_error(Y_test,y_pred_lasso)
RMSE_lasso=np.sqrt(mean_squared_error(Y_test,y_pred_lasso))
r_squr_lasso=r2_score(Y_test,y_pred_lasso)

# create dataframe to view above evaluation metrics
lasso = pd.DataFrame({
    "model": ["LassoRegressor"],
    "MAE": [MAE_lasso],
    "MSE": [MSE_lasso],
    "RMSE": [RMSE_lasso],
    "R2": [r_squr_lasso]
})


ridge = pd.DataFrame({
    "model": ["RidgeRegressor"],
    "MAE": [MAE_ridge],
    "MSE": [MSE_ridge],
    "RMSE": [RMSE_ridge],
    "R2": [r_squr_ridge]
})

# Find the model with the highest RÂ² score
best_model_df = pd.concat([results_df,lasso,ridge], ignore_index=True)
best_model_df

#convert dataframe to csv
path=r"D:\Car_Dekho\Car_Dekho\ref.csv"
best_model_df.to_csv(path,index=False)

best_model = best_model_df.loc[best_model_df['R2'].idxmax()]
print("Best model with high r2 score")
best_model

"""# Hyperparameter tuning for the best **model**"""

# parameters for model
from sklearn.model_selection import RandomizedSearchCV

param_grid = {
    'n_estimators' : [100, 150, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None],
    'bootstrap': [True, False]
}

# tuning the model using grid search cv
grid_tuning=RandomizedSearchCV(estimator=RandomForestRegressor(),
                            param_distributions=param_grid, cv=5, n_jobs=-1)

grid_tuning.fit(X_train,Y_train)

grid_tuning.best_params_,grid_tuning.best_score_

# final model fit and prediction
final_model=RandomForestRegressor(n_estimators=200,max_features=None,max_depth=20,min_samples_leaf=4,
                                  min_samples_split=5,random_state=42)

final_model.fit(X_train,Y_train)

#Model evaluation
y_pred_fm=final_model.predict(X_test)
MSE_fm=mean_squared_error(Y_test,y_pred_fm)
MAE_fm=mean_absolute_error(Y_test,y_pred_fm)
RMSE_fm=np.sqrt(mean_squared_error(Y_test,y_pred_fm))
r_squr_fm=r2_score(Y_test,y_pred_fm)

print(f"Trainscore: {final_model.score(X_train, Y_train)}")
print(f"Testscore: {final_model.score(X_test, Y_test)}")

# create dataframe to view above evaluation metrics
rf_1=pd.DataFrame({"model":"RandomForestRegressor","MAE":[MAE_fm],"MSE":[MSE_fm],"RMSE":[RMSE_fm],"R2":r_squr_fm})
rf_1

"""# store the model using pickle"""

import pickle

# Save the model
with open('Randomforest_regression.pkl', 'wb') as files:
    pickle.dump(final_model, files)  # Replace `final_model` with your trained model object


# load the model,scaler and encoder
with open('Randomforest_regression.pkl','rb') as files:
    final_model=pickle.load(files)

with open('standard.pkl','rb') as f:
    scaler=pickle.load(f)

with open('encoder.pkl','rb') as file:
    encoder=pickle.load(file)

"""# model system buliding-**Pipeline**"""

from google.colab import files
uploaded = files.upload()  # Prompts file upload


import pandas as pd
df_1 = pd.read_csv('final_model.csv')  # Replace with the uploaded file name
print(df_1.head())

df_1

# x and y data splitting
x_df=df_1.drop(columns=["price"],axis=1)
y_df=df_1["price"]

x_df_train,x_df_test,y_df_train,y_df_test=train_test_split(x_df,y_df,random_state=42)

# building pipeline for categorical and numerical data
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Assuming x_df is your DataFrame for features
num = x_df.select_dtypes(include=["int", "float"]).columns.to_list()
cat = x_df.select_dtypes(include=["object"]).columns.to_list()

# Define preprocessors
catg_preprocessor = Pipeline(steps=[
    ("categorical", OneHotEncoder(handle_unknown='ignore'))  # Define the encoder here
])

num_preprocessor = Pipeline(steps=[
    ("numerical", StandardScaler())  # Define the scaler here
])

# Combine preprocessors into a ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ("num", num_preprocessor, num),
        ("cat", catg_preprocessor, cat)
    ]
)

print("Preprocessing pipeline defined successfully!")

# compose preprocessing techniques
preprocessor=ColumnTransformer([("numerical",num_preprocessor,num),
                                ("categorical",catg_preprocessor,cat)])
preprocessor

"""In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
"""

#construct pipeline to combine preprocessor and model
pipeline=Pipeline([("preprocessing",preprocessor),
                   ("model",final_model)])
pipeline

"""Writting again Full code"""

from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Encode the target variable
y_df_encoded = label_encoder.fit_transform(y_df)

# Now you can proceed with splitting or further processing
print("Target variable encoded successfully!")

# Split the data into X (features) and y (target)
x_df = df_1.drop(columns=["price"], axis=1)
y_df = df_1["price"]

# Encode the target variable
y_df_encoded = label_encoder.fit_transform(y_df)

# Split the data into training and testing sets
x_df_train, x_df_test, y_df_train, y_df_test = train_test_split(
    x_df, y_df_encoded, test_size=0.2, random_state=42
)

# Identify numerical and categorical columns
num = x_df.select_dtypes(include=["int", "float"]).columns.to_list()
cat = x_df.select_dtypes(include=["object"]).columns.to_list()

# Preprocessing for numerical and categorical data
catg_preprocessor = Pipeline(
    steps=[("categorical", OneHotEncoder(handle_unknown='ignore', sparse_output=False))]
)
num_preprocessor = Pipeline(steps=[("numerical", StandardScaler())])

# Combine preprocessing techniques
preprocessor = ColumnTransformer(
    [
        ("numerical", num_preprocessor, num),
        ("categorical", catg_preprocessor, cat),
    ]
)

# Construct the full pipeline
pipeline = Pipeline([
    ("preprocessing", preprocessor),
    ("model", final_model)
])

# Fit the pipeline with training data
pipeline.fit(x_df_train, y_df_train)

# evaluate the pipeline
y_prediction=pipeline.predict(x_df_test)

MSE_pipe=mean_squared_error(y_df_test,y_prediction)

MAE_pipe=mean_absolute_error(y_df_test,y_prediction)

RMSE_pipe=np.sqrt(mean_squared_error(y_df_test,y_prediction))

r_squr_pipe=r2_score(y_df_test,y_prediction)

# create dataframe to view above evaluation metrics
Rf_Pipe=pd.DataFrame({"model":"RandomForestRegressor","MAE":[MAE_pipe],"MSE":[MSE_pipe],"RMSE":[RMSE_pipe],"R2":r_squr_pipe})
Rf_Pipe

x_df.columns

x_df["Brand"].unique()

df_1[df_1["price"]>45]

df_1.loc[24]

x_df[x_df["Brand"]=="BMW"]

df_1.iloc[20]

"""# Model prediction"""

# New data to predict car price
new_df=pd.DataFrame({
    'Fuel type':'Disel',
    'body type':'Sedan',
    'transmission':'Automatic',
    'ownerNo':3,
    'Brand':'BMW',
    'model':'BMW 5 Series',
    'modelYear':2011,
    'Insurance Validity': 'Third Party insurance',
    'Kms Driven':100000.0,
    'Mileage':18,
    'Seats':5,
    'Color':'White',
    'City':'Bangalore'
},index=[0])
new_df

# FINAL MODEL PREDICTION
prediction=pipeline.predict(new_df)
print(f"The price of the {new_df['Brand'].iloc[0]} car is: {round(prediction[0],2)} lakhs")

import os
print("Current working directory:", os.getcwd())

import os
print("Files in current directory:", os.listdir('/content'))

import pickle
from sklearn.preprocessing import OneHotEncoder

# Example encoder
encoder = OneHotEncoder()
encoder.fit(X_train)  # Replace X_train with your data


with open('/content/Randomforest_regression.pkl', 'wb') as files:
    pickle.dump(final_model, files)

with open('/content/standard.pkl', 'wb') as f:
    pickle.dump(scaler, f)

with open('/content/encoder.pkl', 'wb') as file:
    pickle.dump(encoder, file)

from google.colab import drive
drive.mount('/content/drive')

with open('/content/drive/MyDrive/Randomforest_regression.pkl', 'wb') as files:
    pickle.dump(final_model, files)

with open('/content/drive/MyDrive/standard.pkl', 'wb') as f:
    pickle.dump(scaler, f)

with open('/content/drive/MyDrive/encoder.pkl', 'wb') as file:
    pickle.dump(encoder, file)

print("Model saved at: /content/Randomforest_regression.pkl")
print("Scaler saved at: /content/standard.pkl")
print("Encoder saved at: /content/encoder.pkl")

import pickle

# Define the full path where you want to save the pickle file
save_path = r"D:\Car_Dekho\pipeline.pkl"  # Replace 'YourFolderName' with your desired directory

# Save the pipeline in pickle
with open(save_path, 'wb') as files:
    pickle.dump(pipeline, files)

print(f"Pipeline saved successfully at {save_path}")

import os
print(f"Current working directory: {os.getcwd()}")

import pickle
from google.colab import files

# Save the pipeline in Colab
save_path = "/content/pipeline.pkl"
with open(save_path, 'wb') as file:
    pickle.dump(pipeline, file)

print(f"Pipeline saved successfully at {save_path}")

# Download the file
files.download(save_path)

